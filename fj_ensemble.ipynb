{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fj_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPY2UWT74wYVLf3gDhzcSYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/r-dube/fakejobs/blob/main/fj_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFWTUL3GCHsG"
      },
      "source": [
        "# Load the modules used\n",
        "import numpy as np\n",
        "import scipy as sci\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, GlobalMaxPool1D, Input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant \n",
        "from keras.optimizers import Adam\n",
        "from keras import metrics\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK7zNIN2x-DP",
        "outputId": "6a10a3b1-5312-48a7-e75c-158225b43862"
      },
      "source": [
        "# NLTK to remove stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nch9McXo45dY",
        "outputId": "d5ef3794-4a4f-4800-ae32-8817985578f8"
      },
      "source": [
        "# list devices\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11792828021395407605\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55AqcsYyb-aa"
      },
      "source": [
        "# For reproducible results\n",
        "# except for variability introduced by GPU\n",
        "import random as rn\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '42'\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # avoid using GPU for reproducible results\n",
        "np.random.seed(42)\n",
        "rn.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFdTIEZ6Z8ZR"
      },
      "source": [
        "# For transformers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrnTamUHDzxI"
      },
      "source": [
        "# Set data_url, the location of the data\n",
        "# Data is not loaded from a local file\n",
        "# data_url=\"https://raw.githubusercontent.com/r-dube/fakejobs/main/data/fj_small.csv\"\n",
        "# data_url=\"https://raw.githubusercontent.com/r-dube/fakejobs/main/data/fj_medium.csv\"\n",
        "data_url=\"https://raw.githubusercontent.com/r-dube/fakejobs/main/data/fake_job_postings.csv\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usYPD_l1Bimz"
      },
      "source": [
        "def fj_load_df_from_url():\n",
        "    \"\"\"\n",
        "    Load dataframe from csv file\n",
        "    Input:\n",
        "        None\n",
        "    Returns:\n",
        "        dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(data_url)\n",
        "\n",
        "    print ('Loaded dataframe shape', df.shape)\n",
        "\n",
        "    counts = fj_label_stats(df)\n",
        "    print ('Not fraudulent', counts[0], 'Fraudulent', counts[1])\n",
        "\n",
        "    print(df.describe())\n",
        "\n",
        "    print ('NAs/NANs in data =>')\n",
        "    print(df.isna().sum())\n",
        "\n",
        "    return df\n",
        "\n",
        "def fj_label_stats(df):\n",
        "    \"\"\"\n",
        "    Very basic label statistics\n",
        "    Input: \n",
        "        Dataframe\n",
        "    Returns:\n",
        "        Number of samples with 0, 1 as the label\n",
        "    \"\"\"\n",
        "    counts = np.bincount(df['fraudulent'])\n",
        "    return counts\n",
        "\n",
        "def fj_txt_only(df):\n",
        "    \"\"\"\n",
        "    Combine all the text fields, discard everything else except for the label\n",
        "    Input: \n",
        "        Dataframe\n",
        "    Returns:\n",
        "        Processed dataframe\n",
        "    \"\"\"\n",
        "    \n",
        "    df.fillna(\" \", inplace = True)\n",
        "\n",
        "    df['text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + \\\n",
        "    ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + \\\n",
        "    df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + \\\n",
        "    ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function'] \n",
        "\n",
        "    del df['title']\n",
        "    del df['location']\n",
        "    del df['department']\n",
        "    del df['company_profile']\n",
        "    del df['description']\n",
        "    del df['requirements']\n",
        "    del df['benefits']\n",
        "    del df['employment_type']\n",
        "    del df['required_experience']\n",
        "    del df['required_education']\n",
        "    del df['industry']\n",
        "    del df['function']  \n",
        "    \n",
        "    del df['salary_range']\n",
        "    del df['job_id']\n",
        "    del df['telecommuting']\n",
        "    del df['has_company_logo']\n",
        "    del df['has_questions']\n",
        "\n",
        "    return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lks9Mm0Tc1l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67aa8361-afd9-448e-f752-a400965f91a9"
      },
      "source": [
        "df = fj_load_df_from_url()\n",
        "df = fj_txt_only(df)\n",
        "print('Maximum text length', df['text'].str.len().max())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded dataframe shape (17880, 18)\n",
            "Not fraudulent 17014 Fraudulent 866\n",
            "             job_id  telecommuting  ...  has_questions    fraudulent\n",
            "count  17880.000000   17880.000000  ...   17880.000000  17880.000000\n",
            "mean    8940.500000       0.042897  ...       0.491723      0.048434\n",
            "std     5161.655742       0.202631  ...       0.499945      0.214688\n",
            "min        1.000000       0.000000  ...       0.000000      0.000000\n",
            "25%     4470.750000       0.000000  ...       0.000000      0.000000\n",
            "50%     8940.500000       0.000000  ...       0.000000      0.000000\n",
            "75%    13410.250000       0.000000  ...       1.000000      0.000000\n",
            "max    17880.000000       1.000000  ...       1.000000      1.000000\n",
            "\n",
            "[8 rows x 5 columns]\n",
            "NAs/NANs in data =>\n",
            "job_id                     0\n",
            "title                      0\n",
            "location                 346\n",
            "department             11547\n",
            "salary_range           15012\n",
            "company_profile         3308\n",
            "description                1\n",
            "requirements            2695\n",
            "benefits                7210\n",
            "telecommuting              0\n",
            "has_company_logo           0\n",
            "has_questions              0\n",
            "employment_type         3471\n",
            "required_experience     7050\n",
            "required_education      8105\n",
            "industry                4903\n",
            "function                6455\n",
            "fraudulent                 0\n",
            "dtype: int64\n",
            "Maximum text length 14991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxtcnlHBpPro"
      },
      "source": [
        "# Utilities to clean text\n",
        "\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
        "    return url.sub(r\"\", text)\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r\"<.*?>\")\n",
        "    return html.sub(r\"\", text)\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE,\n",
        "    )\n",
        "    return emoji_pattern.sub(r\"\", string)\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(table)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgdEvka4wERJ"
      },
      "source": [
        "stop = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "\n",
        "    return \" \".join(text)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrqwWWQ2uruM"
      },
      "source": [
        "# clean text\n",
        "df['text'] = df['text'].map(lambda x: remove_URL(x))\n",
        "df['text'] = df['text'].map(lambda x: remove_html(x))\n",
        "df['text'] = df['text'].map(lambda x: remove_emoji(x))\n",
        "df['text'] = df['text'].map(lambda x: remove_punct(x))\n",
        "df['text'] = df[\"text\"].map(remove_stopwords)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUCnYLkBpnNu"
      },
      "source": [
        "# train-test split\n",
        "train_text, test_text, train_labels , test_labels = train_test_split(df['text'], df['fraudulent'] , test_size = 0.15)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouNER3B_p9cq",
        "outputId": "0cc4b03b-535e-410d-a291-fc4eca95a56d"
      },
      "source": [
        "# Max number of words in a sequence\n",
        "maxlen = 250\n",
        "\n",
        "# embedding size to be created\n",
        "# This depends on the GLOVE file loaded earlier\n",
        "embed_dim = 50\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index)\n",
        "print('Found %s unique tokens.' % vocab_size)\n",
        "vocab_size = vocab_size + 1\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "train_padded = pad_sequences(\n",
        "    train_sequences, maxlen=maxlen, padding=\"post\", truncating=\"post\"\n",
        ")\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
        "test_padded = pad_sequences(\n",
        "    test_sequences, maxlen=maxlen, padding=\"post\", truncating=\"post\"\n",
        ")\n",
        "\n",
        "print(f\"Shape of train {train_padded.shape}\")\n",
        "print(f\"Shape of test {test_padded.shape}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 157210 unique tokens.\n",
            "Shape of train (15198, 250)\n",
            "Shape of test (2682, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsdigN5xaLQL"
      },
      "source": [
        "# Implement multi head self attention as a Keras layer\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Mek1GxaZPP"
      },
      "source": [
        "# Implement a Transformer block as a layer\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h77Q-FsqadIf"
      },
      "source": [
        "# Implement embedding layer\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FDFIIIVasGT"
      },
      "source": [
        "# Create classifier model using transformer layer\n",
        "# embed_dim = 32 # defined above  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model3 = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8W0zyzna5OB",
        "outputId": "ed6b8880-f395-4411-b6a8-d8b9f0c138fe"
      },
      "source": [
        "model3.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\", metrics.FalsePositives(), metrics.FalseNegatives()])\n",
        "model3.summary()\n",
        "model3.fit(train_padded, train_labels, epochs=5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 250)]             0         \n",
            "_________________________________________________________________\n",
            "token_and_position_embedding (None, 250, 50)           7873050   \n",
            "_________________________________________________________________\n",
            "transformer_block (Transform (None, 250, 50)           13682     \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 20)                1020      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 7,887,773\n",
            "Trainable params: 7,887,773\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "475/475 [==============================] - 109s 225ms/step - loss: 0.1767 - accuracy: 0.9406 - false_positives: 57.8214 - false_negatives: 275.6218\n",
            "Epoch 2/5\n",
            "475/475 [==============================] - 109s 230ms/step - loss: 0.0245 - accuracy: 0.9913 - false_positives: 24.1113 - false_negatives: 38.8319\n",
            "Epoch 3/5\n",
            "475/475 [==============================] - 109s 230ms/step - loss: 0.0073 - accuracy: 0.9977 - false_positives: 6.2521 - false_negatives: 8.5294\n",
            "Epoch 4/5\n",
            "475/475 [==============================] - 110s 231ms/step - loss: 7.2255e-04 - accuracy: 0.9999 - false_positives: 0.0378 - false_negatives: 0.7794\n",
            "Epoch 5/5\n",
            "475/475 [==============================] - 110s 231ms/step - loss: 1.5819e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4d0863a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEcsSX4-rko3"
      },
      "source": [
        "pred_soft3 = model3.predict(test_padded)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuNgZv8zXdCD",
        "outputId": "7d6db49c-4b31-4c5d-ebcd-37d60dc13926"
      },
      "source": [
        "# pred = np.around(pred_soft, decimals = 0)\n",
        "pred3 = np.where(pred_soft3 > 0.50, 1, 0)\n",
        "\n",
        "acc3 = accuracy_score(pred3, test_labels)\n",
        "f13 = f1_score(pred3, test_labels)\n",
        "\n",
        "cm3 = confusion_matrix(test_labels, pred3)\n",
        "tn3 = cm3[0][0]\n",
        "fn3 = cm3[1][0]\n",
        "tp3 = cm3[1][1]\n",
        "fp3 = cm3[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(acc3), 'F1 score: {:.4f}'.format(f13))\n",
        "print('False Positives: {:.0f}'.format(fp3), 'False Negatives: {:.0f}'.format(fn3))\n",
        "print('Confusion matrix:\\n', cm3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9851 F1 score: 0.8413\n",
            "False Positives: 2 False Negatives: 38\n",
            "Confusion matrix:\n",
            " [[2536    2]\n",
            " [  38  106]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xipPlzz2Azoz",
        "outputId": "182ba6b3-ef0a-45f4-aa8c-90f6893ec2d6"
      },
      "source": [
        "# model 2: the LSTM model\n",
        "model2 = Sequential()\n",
        "\n",
        "# embed_dim = 50\n",
        "hidden_size = 32\n",
        "model2.add(Embedding(vocab_size, embed_dim, input_length=maxlen))\n",
        "model2.add(Bidirectional(LSTM(hidden_size, dropout=0.1, recurrent_dropout=0.1, return_sequences=True)))\n",
        "model2.add(GlobalMaxPool1D())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', metrics.FalsePositives(), metrics.FalseNegatives()])\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 250, 50)           7860550   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 250, 64)           21248     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 7,881,863\n",
            "Trainable params: 7,881,863\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNhgjKQRByGN",
        "outputId": "7cc9be9f-2e72-4702-be79-ab78cc3b2b5d"
      },
      "source": [
        "model2.fit(train_padded, train_labels, epochs=5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "475/475 [==============================] - 247s 509ms/step - loss: 0.1730 - accuracy: 0.9425 - false_positives_1: 51.4517 - false_negatives_1: 274.0546\n",
            "Epoch 2/5\n",
            "475/475 [==============================] - 241s 508ms/step - loss: 0.0246 - accuracy: 0.9923 - false_positives_1: 16.8571 - false_negatives_1: 40.5966\n",
            "Epoch 3/5\n",
            "475/475 [==============================] - 241s 507ms/step - loss: 0.0036 - accuracy: 0.9986 - false_positives_1: 3.1239 - false_negatives_1: 6.9895\n",
            "Epoch 4/5\n",
            "475/475 [==============================] - 240s 505ms/step - loss: 0.0016 - accuracy: 0.9995 - false_positives_1: 2.4475 - false_negatives_1: 2.6345\n",
            "Epoch 5/5\n",
            "475/475 [==============================] - 240s 506ms/step - loss: 2.7436e-04 - accuracy: 1.0000 - false_positives_1: 0.0000e+00 - false_negatives_1: 0.4244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4d048741d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X78drq-zCybZ"
      },
      "source": [
        "pred_soft2 = model2.predict(test_padded)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuFK-r5uC09H",
        "outputId": "22c9c619-22ca-444b-a60b-0b8fca22124e"
      },
      "source": [
        "# pred = np.around(pred_soft, decimals = 0)\n",
        "pred2 = np.where(pred_soft2 > 0.50, 1, 0)\n",
        "\n",
        "acc2 = accuracy_score(pred2, test_labels)\n",
        "f12 = f1_score(pred2, test_labels)\n",
        "\n",
        "cm2 = confusion_matrix(test_labels, pred2)\n",
        "tn2 = cm2[0][0]\n",
        "fn2 = cm2[1][0]\n",
        "tp2 = cm2[1][1]\n",
        "fp2 = cm2[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(acc2), 'F1 score: {:.4f}'.format(f12))\n",
        "print('False Positives: {:.0f}'.format(fp2), 'False Negatives: {:.0f}'.format(fn2))\n",
        "print('Confusion matrix:\\n', cm2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9870 F1 score: 0.8679\n",
            "False Positives: 6 False Negatives: 29\n",
            "Confusion matrix:\n",
            " [[2532    6]\n",
            " [  29  115]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJIwNfj9XR_p",
        "outputId": "bab06bb9-af20-4825-993b-93497a2ce70e"
      },
      "source": [
        "# model 1: BOW + FCNN model\n",
        "cv = CountVectorizer(strip_accents='unicode', lowercase=True, stop_words='english', dtype=np.int8) \n",
        "cv_train_sparse = cv.fit_transform(train_text)\n",
        "cv_train_dense = sci.sparse.csr_matrix.todense(cv_train_sparse)\n",
        "\n",
        "cv_test_sparse = cv.transform(test_text)\n",
        "cv_test_dense = sci.sparse.csr_matrix.todense(cv_test_sparse)\n",
        "\n",
        "print('BOW for cv_train:', cv_train_dense.shape)\n",
        "print('BOW for cv_test:', cv_test_dense.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW for cv_train: (15198, 150190)\n",
            "BOW for cv_test: (2682, 150190)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGWfDG-KXg5-",
        "outputId": "19c85640-52dd-413c-d0f3-52e0abb8e7fe"
      },
      "source": [
        "\"\"\"\n",
        "Fully connected NN model with two hidden layers \n",
        "\"\"\"\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(units = 100 , activation = 'relu' , input_dim = cv_train_dense.shape[1]))\n",
        "model1.add(Dropout(0.1))\n",
        "model1.add(Dense(units = 10 , activation = 'relu'))\n",
        "model1.add(Dropout(0.1))\n",
        "model1.add(Dense(units = 1 , activation = 'sigmoid'))\n",
        "model1.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy', tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n",
        "model1.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 100)               15019100  \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 15,020,121\n",
            "Trainable params: 15,020,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--t8kVV5XlBy",
        "outputId": "fc3aa982-a3a2-482f-b65e-bf5b1e589ee3"
      },
      "source": [
        "model1.fit(cv_train_dense, train_labels, epochs = 5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "475/475 [==============================] - 54s 111ms/step - loss: 0.1433 - accuracy: 0.9483 - false_positives_2: 43.7752 - false_negatives_2: 251.7605\n",
            "Epoch 2/5\n",
            "475/475 [==============================] - 52s 109ms/step - loss: 0.0176 - accuracy: 0.9940 - false_positives_2: 12.1345 - false_negatives_2: 31.6008\n",
            "Epoch 3/5\n",
            "475/475 [==============================] - 52s 109ms/step - loss: 0.0057 - accuracy: 0.9984 - false_positives_2: 2.9454 - false_negatives_2: 8.9202\n",
            "Epoch 4/5\n",
            "475/475 [==============================] - 52s 110ms/step - loss: 0.0025 - accuracy: 0.9993 - false_positives_2: 2.0819 - false_negatives_2: 3.8655\n",
            "Epoch 5/5\n",
            "475/475 [==============================] - 52s 110ms/step - loss: 0.0021 - accuracy: 0.9992 - false_positives_2: 4.9202 - false_negatives_2: 2.2122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4d02c8a0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67hb8V7sXlfK"
      },
      "source": [
        "pred_soft1 = model1.predict(cv_test_dense)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLIwh073XpxW",
        "outputId": "62063250-28da-446e-fdea-0d90aed61837"
      },
      "source": [
        "# pred = np.around(pred_soft, decimals = 0)\n",
        "pred1 = np.where(pred_soft1 > 0.50, 1, 0)\n",
        "\n",
        "acc1 = accuracy_score(pred1, test_labels)\n",
        "f11 = f1_score(pred1, test_labels)\n",
        "\n",
        "cm1 = confusion_matrix(test_labels, pred1)\n",
        "tn1 = cm1[0][0]\n",
        "fn1 = cm1[1][0]\n",
        "tp1 = cm1[1][1]\n",
        "fp1 = cm1[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(acc1), 'F1 score: {:.4f}'.format(f11))\n",
        "print('False Positives: {:.0f}'.format(fp1), 'False Negatives: {:.0f}'.format(fn1))\n",
        "print('Confusion matrix:\\n', cm1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9862 F1 score: 0.8538\n",
            "False Positives: 1 False Negatives: 36\n",
            "Confusion matrix:\n",
            " [[2537    1]\n",
            " [  36  108]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBvLWsMbYgLj",
        "outputId": "22d99d5b-df0a-483c-dcd9-77404b5a62a4"
      },
      "source": [
        "# Averaging ensemble prediction\n",
        "pred_softa=(pred_soft1 + pred_soft2 + pred_soft3)/3\n",
        "\n",
        "# threshold for averaging ensemble\n",
        "threshold = 0.15\n",
        "\n",
        "# Set probability to declare post as fraudulent\n",
        "preda = np.where(pred_softa > threshold, 1, 0)\n",
        "\n",
        "acca = accuracy_score(preda, test_labels)\n",
        "f1a = f1_score(preda, test_labels)\n",
        "\n",
        "cma = confusion_matrix(test_labels, preda)\n",
        "tna = cma[0][0]\n",
        "fna = cma[1][0]\n",
        "tpa = cma[1][1]\n",
        "fpa = cma[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(acca), 'F1 score: {:.4f}'.format(f1a))\n",
        "print('False Positives: {:.0f}'.format(fpa), 'False Negatives: {:.0f}'.format(fna))\n",
        "print('Confusion matrix:\\n', cma)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9888 F1 score: 0.8913\n",
            "False Positives: 9 False Negatives: 21\n",
            "Confusion matrix:\n",
            " [[2529    9]\n",
            " [  21  123]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i1DqfiT3S1g",
        "outputId": "21e5c4c4-1736-4cc0-b482-6ec48fef0aa6"
      },
      "source": [
        "# print out a couple of false negatives in the test set\n",
        "\n",
        "test_len = preda.shape[0]\n",
        "original = np.array(test_labels)\n",
        "original = original.reshape((test_len, 1))\n",
        "\n",
        "# find index of the first false negative\n",
        "result = np.where((preda == 0) & (original == 1))\n",
        "# result = np.where((test_labels == 1))\n",
        "print (\"False negative indices: \", result)\n",
        "\n",
        "# lookup the index for the first false negative in test_text\n",
        "first_fn = result[0][0]\n",
        "print (\"First false negative:\", first_fn, \"prediction\", preda[first_fn], \"label\", original[first_fn])\n",
        "print (\"Modified job description: \", test_text.iloc[first_fn])\n",
        "\n",
        "# lookup the index for the second false negative in test_text\n",
        "second_fn = result[0][1]\n",
        "print (\"Second false negative:\", second_fn, \"prediction\", preda[second_fn], \"label\", original[second_fn])\n",
        "print (\"Modified job description: \", test_text.iloc[second_fn])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False negative indices:  (array([  18,   75,  464,  536,  663,  733,  828, 1001, 1194, 1343, 1467,\n",
            "       1650, 1701, 1720, 1805, 2106, 2115, 2152, 2352, 2375, 2646]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
            "First false negative: 18 prediction [0] label [1]\n",
            "Modified job description:  urgent requirement position technical lead rhomobile technical mobility lead developer us ca 1 technical lead rhomobile technical mobility lead developer notes case rhomobile difficult find please find people expertise android development dojo css location san francisco san ramon caduration 1 year required competencies around 610 yrs experience minimum 2yrs tech lead mobile projectsexperience rhomobile platform atleast 6 monthsshould experienced cross platform mobile appsproficiency atleast one native platform – ios androidhandson experience essentialshould able lead team guide technical challengesshould able estimate plan various activitiesintegration experience backend systems essential sap integration experience desirableesri arcgis knowledge essential thanks regards jsandeeptechnical recruitertekwissen llc w phoneb464fe6050e48f0c36d00501265378e9581d5d65c73f8e39865543c69aaab557 desk phone46ed5da44d683bbb700c54f51cd225fc80203b64e1c674f7e6d9f826a0223f31 ext 299 fax phonec92713f1c7155e946cc5b07c854ce554f3c95f79f2cbd98500e3cfa2db4ba406 email5f904f421f564976259f740a5c3ee9868cab5264ce252cf686be79da28bffd45 321 main street suite 300 ann arbor mi – 48104 url3b7a90492e728246f2b56db07b079519d22b68faf5d8bbbd4e11a827db01e3ea requirement jd drive mobile app development workgather requirements client’s product management teamsuggesting right architecture adoptedtechnical design appcoordinate offshore team get work done offshoreshould able architect design code deploy applications contract bachelors degree information technology services information technology\n",
            "Second false negative: 75 prediction [0] label [1]\n",
            "Modified job description:  resources change management process excellence change enablement manager us tx houston change management professionals provide knowledge experience related skills structures tools support mechanisms needed manage change foster environment conducive sustaining changeadapts existing methods procedures create possible alternative solutions moderately complex problemsunderstands strategic direction set senior management relates team goalsuses considerable judgment determine solution seeks guidance complex problemsprimary upward interaction direct supervisormay interact peers andor management levels client andor within accenturedetermines methods procedures new assignments guidancedecisions often impact team residemanages small teams andor work efforts individual contributor role client within accentureadditional responsibilities least one following change management offering areaschange strategy· application change architectures models frameworks used execute global multipolar multiworkforce crossgenerational multicultural complex change successfully· assist client executives communicate frameworks cultural norms stakeholder engagement practicesorganization change enablement· approach change management science instead art using datadriven predictable methodology· utilize methods estimators frameworks integrate tightly broader project· drive stakeholder engagement leadership alignment impact analysis learningtraining communications business readiness deployment adoption measurement· help realign organization people actions critical business imperatives specific objectivesorganizational change capability· support client’s achieving sustainable performance andor improvements within organizations· assist client’s building strong change management capability within workforce· assist workforce transition internally managed operations outsourced operations· identify opportunities provide create additional client value· develop trusted relationships key clients internal customers · ability meet travel requirements 100 basic qualifications· minimum 4 years consulting experience relevant experience related successful delivery change management work disciplines change management methodology jobroleorganization design stakeholder identification engagement sponsorship alignment marketing communication trainingperformance support organizational readiness transition outsourcing· bachelors degreepreferred skills· background individual andor organizational psychology· experience leading least one person· experience working offshore thirdparty vendors· experience global cross location multinational projects· experience delivery role complex integrated environment· capable meeting senior executives directors vice presidents· skills hands experience implementing portions largescale erp organizational change programs related enabling systems process change may include limited to· change network strategy design implementation· organizational impacts identification mitigation plans· organization alignment· role mapping· super power user strategy design implementation· skills experiences related talent amp organization performance areas human capital amp organizational effectiveness learning amp collaboration human resource talent management· experience using process mapping training development webpage development applications· experience project management work planning status reporting issue risk management estimating· strong proficiency using microsoft office products eg word excel powerpointprofessional skill requirements· proven success contributing teamoriented environment· proven ability work creatively analytically problemsolving environment· desire work information systems environment· excellent leadership communication written oral interpersonal skillsall consulting professionals receive comprehensive training covering business acumen technical professional skills development you’ll also opportunities hone functional skills expertise area specialization offer variety formal informal training programs every level help acquire build specialized skills faster learning takes place job formal training conducted online classroom collaboration teammates sheer variety work experience offers provide unbeatable platform build career fulltime oil energy human resources\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCEoMit7nNGe"
      },
      "source": [
        "# stacking ensemble\n",
        "\n",
        "# get prediction scores for train samples\n",
        "train_soft1 = model1.predict(cv_train_dense)\n",
        "train_soft2 = model2.predict(train_padded)\n",
        "train_soft3 = model3.predict(train_padded)\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3UDyxO-ua_Q"
      },
      "source": [
        "stack_train = np.hstack([train_soft1, train_soft2, train_soft3])\n",
        "\n",
        "stack_test = np.hstack([pred_soft1, pred_soft2, pred_soft3])\n",
        "\n",
        "model_stack = LogisticRegression()\n",
        "model_stack.fit(stack_train, train_labels)\n",
        "\n",
        "pred_softs = model_stack.predict(stack_test)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRXDpGvVsM6q",
        "outputId": "8140da71-19f0-4ff9-9f6b-fd306ee8ca04"
      },
      "source": [
        "# Stacking prediction\n",
        "\n",
        "# Set probability to declare post as fraudulent\n",
        "preds = np.where(pred_softs > 0.01, 1, 0)\n",
        "\n",
        "accs = accuracy_score(preds, test_labels)\n",
        "f1s = f1_score(preds, test_labels)\n",
        "\n",
        "cms = confusion_matrix(test_labels, preds)\n",
        "tns = cms[0][0]\n",
        "fns = cms[1][0]\n",
        "tps = cms[1][1]\n",
        "fps = cms[0][1]\n",
        "\n",
        "print('Accuracy score: {:.4f}'.format(accs), 'F1 score: {:.4f}'.format(f1s))\n",
        "print('False Positives: {:.0f}'.format(fps), 'False Negatives: {:.0f}'.format(fns))\n",
        "print('Confusion matrix:\\n', cms)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9858 F1 score: 0.8480\n",
            "False Positives: 0 False Negatives: 38\n",
            "Confusion matrix:\n",
            " [[2538    0]\n",
            " [  38  106]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}