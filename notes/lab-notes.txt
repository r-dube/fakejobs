
12/15/2020
1. Using the transformer model with GPU on colab takes ~38% of the time to run
   Epoch 5/5
   475/475 [==============================] - 39s 82ms/step - loss: 8.3997e-05 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9870 F1 score: 0.8293
   False Positives: 4 False Negatives: 31
   Confusion matrix:
    [[2562    4]
    [  31   85]]
2. Using TPU as the runtime is approximately the same as running with CPUs. Likely some additional configureation needs to be turned on for speedup.
   Epoch 5/5
   475/475 [==============================] - 94s 199ms/step - loss: 0.0023 - accuracy: 0.9992 - false_positives: 7.0000 - false_negatives: 5.0000
   Accuracy score: 0.9855 F1 score: 0.8571
   False Positives: 16 False Negatives: 23
   Confusion matrix:
    [[2526   16]
    [  23  117]]
3. It appears os.environ['CUDA_VISIBLE_DEVICES'] = '-1' not os.environ['CUDA_VISIBLE_DEVICES'] = '' is needed to force Colab to use only the CPU, when a GPU is also present
4. Ensemble on medium data set, after setting threshold to 20% (probability of being fake)
   Accuracy score: 0.9832 F1 score: 0.7907
   False Positives: 3 False Negatives: 6
   Confusion matrix:
    [[511   3]
    [  6  17]]
5. With GPU, training LSTM seems to take longer. Reverting to running entire LSTM model on CPU
   Accuracy score: 0.9881 F1 score: 0.8815
   False Positives: 7 False Negatives: 25
   Confusion matrix:
    [[2531    7]
    [  25  119]]

12/12/2020
1. Glove LSTM model needs to be revisited - it fails on syntax currently. Need to rerun final result to confirm finding.
2. First transformer model
   Epoch 5/5
   95/95 [==============================] - 16s 164ms/step - loss: 0.0026 - accuracy: 0.9993 - false_positives_3: 1.0000 - false_negatives_3: 1.0000
   Accuracy score: 0.9851 F1 score: 0.8000
3. After clearing an error from the previous implementation
   Epoch 5/5
   95/95 [==============================] - 16s 173ms/step - loss: 0.0064 - accuracy: 0.9987 - false_positives: 1.0000 - false_negatives: 3.0000
   Accuracy score: 0.9888 F1 score: 0.8571
4. On the full data set
   Epoch 5/5
   475/475 [==============================] - 105s 222ms/step - loss: 1.1689e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9851 F1 score: 0.8413

12/10/2020
1. embed_dim = 50
   hidden_size = 32
   Epoch 5/5
   95/95 [==============================] - 41s 434ms/step - loss: 4.7101e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9907 F1 score: 0.8837
2. Repeated the above experiment but after setting the maximum number of words in the tokenizer to 25000
   Epoch 5/5
   95/95 [==============================] - 42s 440ms/step - loss: 3.3852e-04 - accuracy: 1.0000 - false_positives_1: 0.0000e+00 - false_negatives_1: 0.0000e+00
   Accuracy score: 0.9888 F1 score: 0.8571
3. With glove (code clean up needed), embed_dim = 50
   Epoch 10/10
   95/95 [==============================] - 12s 128ms/step - loss: 0.0082 - accuracy: 0.9990
   Accuracy score: 0.9795 F1 score: 0.7556
4. With cleaned up glove, embed_dim = 50, hidden_size = 32
   Epoch 10/10
   95/95 [==============================] - 13s 136ms/step - loss: 0.0028 - accuracy: 1.0000 - false_positives_1: 0.0000e+00 - false_negatives_1: 0.0000e+00
   Accuracy score: 0.9814 F1 score: 0.7826
5. Glove with full dataset
   Epoch 10/10
   475/475 [==============================] - 64s 134ms/step - loss: 0.0086 - accuracy: 0.9975 - false_positives_2: 13.0000 - false_negatives_2: 25.0000
   Accuracy score: 0.9787 F1 score: 0.7912

12/9/2020
1. Full dataset with hidden_size 32 (takes a long time to run)
   Epoch 5/5
   475/475 [==============================] - 258s 543ms/step - loss: 1.3497e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9873 F1 score: 0.8741
2. On reducing the hidden_size to 16
   Epoch 5/5
   475/475 [==============================] - 176s 370ms/step - loss: 2.5393e-04 - accuracy: 0.9999 - false_positives: 1.0000 - false_negatives: 0.0000e+00
   Accuracy score: 0.9851 F1 score: 0.8413
3. On reducing hidden_size to 8
   Epoch 5/5
   475/475 [==============================] - 146s 307ms/step - loss: 2.5624e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9840 F1 score: 0.8365
4. On moving hidden_size to 32 but training for 3 epochs
   Epoch 3/3
   475/475 [==============================] - 260s 548ms/step - loss: 0.0036 - accuracy: 0.9991 - false_positives: 3.0000 - false_negatives: 11.0000
   Accuracy score: 0.9877 F1 score: 0.8764
5. Separating embedding_dim(16) and hidden_size (16)
   Epoch 5/5
   95/95 [==============================] - 30s 313ms/step - loss: 0.0022 - accuracy: 0.9997 - false_positives_7: 0.0000e+00 - false_negatives_7: 1.0000
   Accuracy score: 0.9758 F1 score: 0.6486
6. embed_dim = 16
   hidden_size = 32
   Epoch 5/5
   95/95 [==============================] - 35s 371ms/step - loss: 0.0028 - accuracy: 0.9997 - false_positives_8: 0.0000e+00 - false_negatives_8: 1.0000
   Accuracy score: 0.9851 F1 score: 0.8095
7. embed_dim = 32
   hidden_size = 16
   Epoch 5/5
   95/95 [==============================] - 32s 334ms/step - loss: 0.0012 - accuracy: 0.9997 - false_positives_9: 1.0000 - false_negatives_9: 0.0000e+00
   Accuracy score: 0.9907 F1 score: 0.8837
8. embed_dim = 32
   hidden_size = 8
   Epoch 5/5
   95/95 [==============================] - 30s 314ms/step - loss: 7.1143e-04 - accuracy: 1.0000 - false_positives_10: 0.0000e+00 - false_negatives_10: 0.0000e+00
   Accuracy score: 0.9907 F1 score: 0.8889
9. Full dataset with above parameters
   Epoch 5/5
   475/475 [==============================] - 169s 355ms/step - loss: 1.5170e-04 - accuracy: 1.0000 - false_positives_11: 0.0000e+00 - false_negatives_11: 0.0000e+00
   Accuracy score: 0.9873 F1 score: 0.8692
10. Full dataset
   embed_dim = 32
   hidden_size = 32
   Epoch 5/5
   475/475 [==============================] - 224s 472ms/step - loss: 8.5368e-05 - accuracy: 1.0000 - false_positives_12: 0.0000e+00 - false_negatives_12: 0.0000e+00
   Accuracy score: 0.9884 F1 score: 0.8812

12/7/2020
1. LTSM network, medium dataset, max_length = 250, embedding_dim = 32, LSTM nodes = 64, dropout = 0.1, learning_rate = 0.0005
   Epoch 5/5
   95/95 [==============================] - 16s 168ms/step - loss: 0.1981 - accuracy: 0.9483 - false_positives: 0.0000e+00 - false_negatives: 157.0000
   Accuracy score: 0.9553 F1 score: 0.0000
2. With full dataset and same network and parameters as above:
   Epoch 5/5
   475/475 [==============================] - 97s 204ms/step - loss: 0.1212 - accuracy: 0.9647 - false_positives: 66.0000 - false_negatives: 471.0000
   Accuracy score: 0.9556 F1 score: 0.3425
   So even with the default network that we started with, some learning is happening if enough data is provided
3. Turning the network into "bidirectional" with 32 nodes and going back to the medium dataset improves performance significantly; lr = 0.01, dropout removed, retrun_sequences=True
   Epoch 5/5
   95/95 [==============================] - 16s 169ms/step - loss: 8.4216e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9832 F1 score: 0.7805
4. With 16 nodes inside bidirection(lstm)
   Epoch 5/5
   95/95 [==============================] - 15s 153ms/step - loss: 6.1672e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9888 F1 score: 0.8571
5. Results remain unchanged even if dropout=0.1 inside LSTM
6. Running with the above setup on the full data set
   Epoch 5/5
   475/475 [==============================] - 95s 199ms/step - loss: 2.3415e-04 - accuracy: 0.9999 - false_positives: 1.0000 - false_negatives: 0.0000e+00
   Accuracy score: 0.9866 F1 score: 0.8615
7. Setting embedding_dim = lstm units = 16, medium dataset
   Epoch 5/5
   95/95 [==============================] - 13s 132ms/step - loss: 0.0018 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9870 F1 score: 0.8293
8. Renamed embedding_dim as hidden_size(16); the embedding output is 2*hidden_size and the LSTM layers take just hidden_size; confirm results on medium dataset
   Epoch 5/5
   95/95 [==============================] - 15s 156ms/step - loss: 6.4386e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9888 F1 score: 0.8571
9. Adding recurrent_dropout makes no difference
10.Increased hidden_size from 16 to 32
   Epoch 5/5
   95/95 [==============================] - 48s 510ms/step - loss: 5.2571e-04 - accuracy: 1.0000 - false_positives: 0.0000e+00 - false_negatives: 0.0000e+00
   Accuracy score: 0.9907 F1 score: 0.8837


12/5/2020
1. Finalized classifier on medium data set
   Epoch 5/5
   95/95 [==============================] - 3s 30ms/step - loss: 0.0026 - accuracy: 0.9997 - false_positives_1: 0.0000e+00 - false_negatives_1: 1.0000
   Accuracy score: 0.9851 F1 score: 0.8095

12/3/2020
1. Attempting to use the count vectorizer (cv) with Google colab (free version) and the full copy of the data crashed the colab instance. Colab reports that the crash was due to all available RAM being used up. 
2. Create a medium (20%) version of the data in an attempt to continue using the existing free setup on Colab.
3. With medium version of the data and the fcnn model
   Epoch 5/5
   95/95 [==============================] - 3s 30ms/step - loss: 2.9882e-04 - accuracy: 1.0000
   Accuracy score: 0.9870 F1 score: 0.8372
4. The full data set works on colab if count vectorizer is forced to use 8-bit integers
   Epoch 5/5
   475/475 [==============================] - 36s 75ms/step - loss: 0.0037 - accuracy: 0.9994
   Accuracy score: 0.9855 F1 score: 0.8561
5. On simplifying the fcnn from four hidden layers to two (input -> 100 -> 10 -> output)
   Epoch 5/5
   475/475 [==============================] - 35s 73ms/step - loss: 0.0025 - accuracy: 0.9995
   Accuracy score: 0.9881 F1 score: 0.8769
6. Use droput at each of the two hidden layers
   Epoch 5/5
   475/475 [==============================] - 34s 71ms/step - loss: 0.0015 - accuracy: 0.9997 - false_positives_10: 2.0000 - false_negatives_9: 2.0000
   Accuracy score: 0.9888 F1 score: 0.8864

12/2/2020
1. A neural network model that uses a basic count vectorizer and an FCNN with fj_small.csv also returns poor results
   Accuracy score: 0.9444444444444444 F1 score: 0.0
2. It is unclear if the poor results are due to the smallness of the dataset, the vectorizer (and parameters passed to it), the lack of other text processing (lemmzatization, stemming) or something else

12/1/2020
1. Very basic logistic regression model that does not use an natural language processing returns poor results on the full data set
   Accuracy score: 0.9478 F1 score: 0.0541
   Confusion matrix:
   [[2538    0]
    [ 140    4]]
