### List of references used in posts
* [^kaggle1]: [Dataset of real and fake job postings](https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction)
* [^kaggle2]: [US Jobs on Monster.com](https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom)
* [^data1]: [Github copy of dataset from Kaggle](https://github.com/r-dube/fakejobs/blob/main/data/fake_job_postings.csv)
* [^data2]: [Medium dataset: 20% of total](https://github.com/r-dube/fakejobs/blob/main/data/fj_medium.csv)
* [^data3]: [Small dataset: 3% of total](https://github.com/r-dube/fakejobs/blob/main/data/fj_small.csv)
* [^script1]: [Script to split the dataset (also contains logistic regression classifier)](https://github.com/r-dube/fakejobs/blob/main/scripts/fj_prep.py)
* [^script2]: [Script to clean data](https://github.com/r-dube/fakejobs/blob/main/scripts/fj_gensim_input.py)
* [^script3]: [Script to generate FastText embedding](https://github.com/r-dube/fakejobs/blob/main/scripts/fj_gensim_fasttext.py)
* [^colab1]: [Bag-of-words with a fully-connected neural network model](https://github.com/r-dube/fakejobs/blob/main/fj_fcnn.ipynb)
* [^colab2]: [Bag-of-words + numerical features with a fully-connected neural network model](https://github.com/r-dube/fakejobs/blob/main/fj_fcnn_num.ipynb)
* [^colab3]: [LSTM model with a word-embedding layer](https://github.com/r-dube/fakejobs/blob/main/fj_lstm.ipynb)
* [^colab4]: [LSTM model with a pre-trained word-embedding layer](https://github.com/r-dube/fakejobs/blob/main/fj_glove_lstm.ipynb)
* [^colab5]: [Transformer model](https://github.com/r-dube/fakejobs/blob/main/fj_transformer.ipynb)
* [^colab6]: [Ensemble models](https://github.com/r-dube/fakejobs/blob/main/fj_ensemble.ipynb)
* [^colab7]: [Logistic regression model with bag-of-words](https://github.com/r-dube/fakejobs/blob/main/fj_bow_logistic.ipynb)
* [^colab8]: [Bag-of-words, neural network model with no hidden layer](https://github.com/r-dube/fakejobs/blob/main/fj_nohidden.ipynb)
* [^colab9]: [ROC curves creation](https://github.com/r-dube/fakejobs/blob/main/fj_roc_auc.ipynb)
* [^colab10]: [N-gram + TF-IDF + Logistic regression model](https://github.com/r-dube/fakejobs/blob/main/fj_ngram_tfidf_logistic.ipynb)
* [^colab11]: [Char CNN model](https://github.com/r-dube/fakejobs/blob/main/fj_char_cnn.ipynb)
* [^colab12]: [Composite char and token model](https://github.com/r-dube/fakejobs/blob/main/fj_composite.ipynb)
* [^colab13]: [Composite char and token model with trainable FastText embedding](https://github.com/r-dube/fakejobs/blob/c7fe25acbe28a08949f8a35003539cf7ee5687a2/fj_embedding_composite.ipynb)
* [^colab13-2]: [Composite char and token model with trainable FastText embedding - updated](https://github.com/r-dube/fakejobs/blob/94eab0adc2e5309d6fdb3a8591abb68a3f16f7d2/fj_embedding_composite.ipynb)
* [^colab13-3]: [Composite char and token model with Word2Vec embedding](https://github.com/r-dube/fakejobs/blob/2a0d8526029085fdd5ae1927754f19e23b6eb1c6/fj_embedding_composite.ipynb)
* [^colab14]: [Colab notebook for Word2Vec embeddings](https://github.com/r-dube/fakejobs/blob/2a0d8526029085fdd5ae1927754f19e23b6eb1c6/fj_gensim_word2vec.ipynb)
* [^glove1]: [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
* [^trans1]: [Transformers from scratch (video lecture)](https://www.youtube.com/playlist?list=PLIXJ-Sacf8u60G1TwcznBmK6rEL3gmZmV)
* [^trans2]: [Transformers from scratch (blog post)](http://peterbloem.nl/blog/transformers)
* [^trans3]: [Text classification with Transformer](https://keras.io/examples/nlp/text_classification_with_transformer/)
* [^ensemble1]: [Ensemble techniques](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/)
* [^wiki1]: [Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
* [^paper1]: [Character-level Convolutional Networks for Text Classification](https://arxiv.org/pdf/1509.01626.pdf)
* [^paper2]: [Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting](https://arxiv.org/pdf/1506.04214.pdf)
* [^paper3]: [AMSI-Based Detection of Malicious PowerShell Code Using Contextual Embeddings](https://arxiv.org/abs/1905.09538)
* [^word2vec1]: [Word2Vec Tutorial - The Skip-Gram Model](ccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
* [^word2vec2]: [Word2vec and friends](https://www.youtube.com/watch?v=wTp3P2UnTfQ)
